[program started on Wed Dec 16 05:01:25 2015] 
[command line arguments] 
[----------------------] 
-------------------------------------------------------------------------------- 
LSTM for VQA 
-------------------------------------------------------------------------------- 
loading COCOQA datasets 
num train = 78736 
num test  = 38948 
loading features 
-------------------------------------------------------------------------------- 
model configuration 
-------------------------------------------------------------------------------- 
max epochs                = 50 
num params                = 222580 
num compositional params  = 157650 
word vector dim           = 50 
LSTM memory dim           = 150 
LSTM structure            = rnnsu 
LSTM layers               = 1 
regularization strength   = 1.00e-04 
minibatch size            = 1 
learning rate             = 5.00e-02 
word vector learning rate = 1.00e-01 
dropout                   = true 
cuda                      = true 
image feature dim         = 1000 
-------------------------------------------------------------------------------- 
Training model 
-------------------------------------------------------------------------------- 
-- epoch 1 
-- finished epoch in 195.28s 
-- train score: 0.48635947978053, cost 368.39s 
-- test score: 0.44877785765636, cost 175.12s 
-- epoch 2 
-- finished epoch in 164.37s 
-- train score: 0.52707782970941, cost 357.23s 
-- test score: 0.47252747252747, cost 176.37s 
-- epoch 3 
-- finished epoch in 166.30s 
-- train score: 0.54747510668563, cost 358.01s 
-- test score: 0.48305432884872, cost 176.30s 
-- epoch 4 
-- finished epoch in 165.73s 
-- train score: 0.56270321072953, cost 362.63s 
-- test score: 0.48754749922974, cost 176.77s 
-- epoch 5 
-- finished epoch in 172.10s 
-- train score: 0.57158097947572, cost 357.53s 
-- test score: 0.49288795316833, cost 176.86s 
-- epoch 6 
-- finished epoch in 167.23s 
-- train score: 0.58706309693152, cost 359.51s 
-- test score: 0.49327308205813, cost 178.05s 
-- epoch 7 
-- finished epoch in 162.54s 
-- train score: 0.60000508026824, cost 363.55s 
-- test score: 0.49306767998357, cost 176.90s 
-- epoch 8 
-- finished epoch in 164.83s 
-- train score: 0.60814621011989, cost 358.04s 
-- test score: 0.49686761836295, cost 179.49s 
-- epoch 9 
-- finished epoch in 168.85s 
-- train score: 0.61784952245479, cost 359.33s 
-- test score: 0.49745814932731, cost 174.83s 
-- epoch 10 
-- finished epoch in 159.46s 
-- train score: 0.62211694777484, cost 357.54s 
-- test score: 0.49812570606963, cost 176.83s 
-- epoch 11 
-- finished epoch in 166.99s 
-- train score: 0.63371266002845, cost 358.44s 
-- test score: 0.49989729896272, cost 174.99s 
-- epoch 12 
-- finished epoch in 163.23s 
-- train score: 0.63845001016054, cost 357.55s 
-- test score: 0.50166889185581, cost 175.24s 
-- epoch 13 
-- finished epoch in 164.36s 
-- train score: 0.6432508636456, cost 357.58s 
-- test score: 0.49812570606963, cost 179.33s 
-- epoch 14 
-- finished epoch in 167.87s 
-- train score: 0.65115068075594, cost 357.88s 
-- test score: 0.50046215466776, cost 176.64s 
-- epoch 15 
-- finished epoch in 167.40s 
-- train score: 0.65484657589921, cost 360.90s 
-- test score: 0.49774057717983, cost 178.25s 
-- epoch 16 
-- finished epoch in 174.48s 
-- train score: 0.66076508839667, cost 359.85s 
-- test score: 0.499358118517, cost 176.89s 
-- epoch 17 
-- finished epoch in 174.79s 
-- train score: 0.66642958748222, cost 359.12s 
-- test score: 0.49709869569683, cost 176.64s 
-- epoch 18 
-- finished epoch in 174.26s 
-- train score: 0.66856330014225, cost 354.52s 
-- test score: 0.50097565985416, cost 175.31s 
-- epoch 19 
-- finished epoch in 166.07s 
-- train score: 0.6725767120504, cost 356.38s 
-- test score: 0.49874191229331, cost 176.37s 
-- epoch 20 
-- finished epoch in 164.48s 
-- train score: 0.67625990652306, cost 357.28s 
-- test score: 0.49817705658827, cost 176.74s 
-- epoch 21 
-- finished epoch in 168.29s 
-- train score: 0.67798719772404, cost 361.83s 
-- test score: 0.49804868029167, cost 175.54s 
-- epoch 22 
-- finished epoch in 166.34s 
-- train score: 0.68426132899817, cost 357.53s 
-- test score: 0.49861353599671, cost 178.19s 
-- epoch 23 
-- finished epoch in 162.92s 
-- train score: 0.68323257467994, cost 354.66s 
-- test score: 0.49763787614255, cost 175.45s 
-- epoch 24 
-- finished epoch in 168.75s 
-- train score: 0.69087837837838, cost 355.29s 
-- test score: 0.49820273184759, cost 175.57s 
-- epoch 25 
-- finished epoch in 167.41s 
-- train score: 0.69198333672018, cost 358.61s 
-- test score: 0.49632843791722, cost 178.38s 
-- epoch 26 
-- finished epoch in 168.25s 
-- train score: 0.6955268238163, cost 358.38s 
-- test score: 0.50020540207456, cost 177.23s 
-- epoch 27 
-- finished epoch in 162.40s 
-- train score: 0.70002286120707, cost 359.89s 
-- test score: 0.4959176337681, cost 178.46s 
-- epoch 28 
-- finished epoch in 163.38s 
-- train score: 0.70392196707986, cost 356.56s 
-- test score: 0.49858786073739, cost 175.38s 
-- epoch 29 
-- finished epoch in 162.75s 
-- train score: 0.70938325543589, cost 354.59s 
-- test score: 0.49599465954606, cost 175.32s 
-- epoch 30 
-- finished epoch in 162.88s 
-- train score: 0.70806238569396, cost 357.43s 
-- test score: 0.49915271644244, cost 177.22s 
-- epoch 31 
-- finished epoch in 167.56s 
-- train score: 0.71259652509653, cost 362.53s 
-- test score: 0.49666221628838, cost 181.83s 
-- epoch 32 
-- finished epoch in 168.57s 
-- train score: 0.71219010363747, cost 359.41s 
-- test score: 0.49892163910855, cost 179.59s 
-- epoch 33 
-- finished epoch in 166.54s 
-- train score: 0.71749898394635, cost 361.16s 
-- test score: 0.49817705658827, cost 176.08s 
-- epoch 34 
-- finished epoch in 160.51s 
-- train score: 0.71553038000406, cost 361.65s 
-- test score: 0.49673924206634, cost 177.67s 
-- epoch 35 
-- finished epoch in 165.40s 
-- train score: 0.71993751270067, cost 360.67s 
-- test score: 0.49589195850878, cost 178.49s 
-- epoch 36 
-- finished epoch in 162.89s 
-- train score: 0.71837533021744, cost 357.93s 
-- test score: 0.49522440176646, cost 178.50s 
-- epoch 37 
-- finished epoch in 173.16s 
-- train score: 0.72722769762243, cost 359.69s 
-- test score: 0.49771490192051, cost 177.88s 
-- epoch 38 
-- finished epoch in 167.30s 
-- train score: 0.72652916073969, cost 360.33s 
-- test score: 0.49581493273082, cost 184.02s 
-- epoch 39 
-- finished epoch in 168.99s 
-- train score: 0.72548770575086, cost 358.60s 
-- test score: 0.49704734517819, cost 176.66s 
-- epoch 40 
-- finished epoch in 168.24s 
-- train score: 0.73042826661248, cost 357.94s 
-- test score: 0.49355550991065, cost 175.63s 
-- epoch 41 
-- finished epoch in 164.97s 
-- train score: 0.72865017272912, cost 364.41s 
-- test score: 0.49373523672589, cost 176.27s 
-- epoch 42 
-- finished epoch in 168.67s 
-- train score: 0.73059337533022, cost 359.19s 
-- test score: 0.49458252028345, cost 176.12s 
-- epoch 43 
-- finished epoch in 159.60s 
-- train score: 0.7292471042471, cost 358.28s 
-- test score: 0.49468522132074, cost 176.25s 
-- epoch 44 
-- finished epoch in 175.15s 
-- train score: 0.73426386913229, cost 359.24s 
-- test score: 0.49430009243093, cost 177.85s 
-- epoch 45 
-- finished epoch in 165.81s 
-- train score: 0.73489890266206, cost 359.05s 
-- test score: 0.49720139673411, cost 177.86s 
-- epoch 46 
-- finished epoch in 167.63s 
-- train score: 0.73547043283885, cost 359.02s 
-- test score: 0.49486494813598, cost 177.82s 
-- epoch 47 
-- finished epoch in 166.11s 
-- train score: 0.73436547449705, cost 359.25s 
-- test score: 0.4966878915477, cost 177.75s 
-- epoch 48 
-- finished epoch in 167.92s 
-- train score: 0.73981406218248, cost 359.07s 
-- test score: 0.49581493273082, cost 175.92s 
-- epoch 49 
-- finished epoch in 168.67s 
-- train score: 0.73748983946352, cost 359.54s 
-- test score: 0.49319605628017, cost 177.96s 
-- epoch 50 
-- finished epoch in 168.79s 
-- train score: 0.7416937614306, cost 356.52s 
-- test score: 0.49417171613433, cost 175.62s 
finished training in 35172.97s 
best dev score is: 0.50166889185581 
writing model to ./done/vqalstm-COCOQA-rnnsu.l1.d150.e12.c1-2015-12-16T144743.t7 
