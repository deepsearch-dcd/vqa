[program started on Thu Dec 17 17:35:30 2015] 
[command line arguments] 
[----------------------] 
-------------------------------------------------------------------------------- 
LSTM for VQA 
-------------------------------------------------------------------------------- 
loading COCOQA datasets 
num train = 78736 
num test  = 38948 
loading features 
-------------------------------------------------------------------------------- 
model configuration 
-------------------------------------------------------------------------------- 
max epochs                = 50 
num params                = 786130 
num compositional params  = 721200 
word vector dim           = 50 
LSTM memory dim           = 150 
LSTM structure            = lstm 
LSTM layers               = 1 
regularization strength   = 1.00e-04 
minibatch size            = 1 
learning rate             = 5.00e-02 
word vector learning rate = 1.00e-01 
dropout                   = true 
cuda                      = true 
image feature dim         = 1000 
-------------------------------------------------------------------------------- 
Training model 
-------------------------------------------------------------------------------- 
-- epoch 1 
-- finished epoch in 701.51s 
-- train score: 0.44199603739077, cost 551.81s 
-- test score: 0.42222963951936, cost 275.54s 
-- epoch 2 
-- finished epoch in 655.39s 
-- train score: 0.48902662060557, cost 553.57s 
-- test score: 0.46038307486906, cost 274.01s 
-- epoch 3 
-- finished epoch in 668.91s 
-- train score: 0.52408047144889, cost 557.06s 
-- test score: 0.48133408647427, cost 272.27s 
-- epoch 4 
-- finished epoch in 655.74s 
-- train score: 0.54718299126194, cost 544.72s 
-- test score: 0.49050015405156, cost 270.67s 
-- epoch 5 
-- finished epoch in 659.21s 
-- train score: 0.56733895549685, cost 546.49s 
-- test score: 0.50141213926261, cost 268.80s 
-- epoch 6 
-- finished epoch in 654.38s 
-- train score: 0.58330369843528, cost 541.59s 
-- test score: 0.50554585601315, cost 267.37s 
-- epoch 7 
-- finished epoch in 649.62s 
-- train score: 0.59784596626702, cost 541.27s 
-- test score: 0.50770257779604, cost 267.51s 
-- epoch 8 
-- finished epoch in 652.45s 
-- train score: 0.61303596829913, cost 541.07s 
-- test score: 0.51075793365513, cost 271.32s 
-- epoch 9 
-- finished epoch in 650.00s 
-- train score: 0.62174862832758, cost 543.82s 
-- test score: 0.51201602136182, cost 270.34s 
-- epoch 10 
-- finished epoch in 653.45s 
-- train score: 0.63066449908555, cost 543.93s 
-- test score: 0.51278627914142, cost 268.79s 
-- epoch 11 
-- finished epoch in 653.85s 
-- train score: 0.63848811217232, cost 543.58s 
-- test score: 0.51371058847694, cost 268.49s 
-- epoch 12 
-- finished epoch in 650.07s 
-- train score: 0.64745478561268, cost 544.21s 
-- test score: 0.51255520180754, cost 268.60s 
-- epoch 13 
-- finished epoch in 649.06s 
-- train score: 0.65832655964235, cost 543.71s 
-- test score: 0.5134024853651, cost 268.78s 
-- epoch 14 
-- finished epoch in 653.74s 
-- train score: 0.66834738874213, cost 543.40s 
-- test score: 0.51453219677519, cost 269.47s 
-- epoch 15 
-- finished epoch in 654.24s 
-- train score: 0.67493903678114, cost 555.64s 
-- test score: 0.51430111944131, cost 269.14s 
-- epoch 16 
-- finished epoch in 655.31s 
-- train score: 0.68428673033936, cost 544.15s 
-- test score: 0.51689432063264, cost 268.99s 
-- epoch 17 
-- finished epoch in 655.29s 
-- train score: 0.68868116236537, cost 545.38s 
-- test score: 0.51745917633768, cost 268.85s 
-- epoch 18 
-- finished epoch in 655.67s 
-- train score: 0.69528551107498, cost 544.25s 
-- test score: 0.51550785662935, cost 270.54s 
-- epoch 19 
-- finished epoch in 656.26s 
-- train score: 0.6996545417598, cost 545.43s 
-- test score: 0.51509705248023, cost 269.00s 
-- epoch 20 
-- finished epoch in 655.76s 
-- train score: 0.70829099776468, cost 544.13s 
-- test score: 0.51602136181575, cost 269.10s 
-- epoch 21 
-- finished epoch in 656.61s 
-- train score: 0.71277433448486, cost 544.40s 
-- test score: 0.51561055766663, cost 269.15s 
-- epoch 22 
-- finished epoch in 659.57s 
-- train score: 0.71567008738061, cost 544.77s 
-- test score: 0.51245250077026, cost 269.35s 
-- epoch 23 
-- finished epoch in 652.64s 
-- train score: 0.72574171916277, cost 544.45s 
-- test score: 0.51730512478176, cost 269.17s 
-- epoch 24 
-- finished epoch in 657.90s 
-- train score: 0.72269355821987, cost 545.20s 
-- test score: 0.51104036150765, cost 267.86s 
-- epoch 25 
-- finished epoch in 655.15s 
-- train score: 0.7361816703922, cost 549.49s 
-- test score: 0.51229844921434, cost 271.09s 
-- epoch 26 
-- finished epoch in 660.87s 
-- train score: 0.73961085145296, cost 544.80s 
-- test score: 0.51037280476533, cost 269.40s 
-- epoch 27 
-- finished epoch in 654.61s 
-- train score: 0.74569447266816, cost 544.27s 
-- test score: 0.51453219677519, cost 269.06s 
-- epoch 28 
-- finished epoch in 656.16s 
-- train score: 0.75030481609429, cost 548.14s 
-- test score: 0.51312005751258, cost 274.52s 
-- epoch 29 
-- finished epoch in 652.56s 
-- train score: 0.75476275147328, cost 542.55s 
-- test score: 0.51188764506522, cost 268.71s 
-- epoch 30 
-- finished epoch in 681.79s 
-- train score: 0.75729018492176, cost 543.92s 
-- test score: 0.51478894936839, cost 268.52s 
-- epoch 31 
-- finished epoch in 656.22s 
-- train score: 0.76057965860597, cost 549.16s 
-- test score: 0.51003902639417, cost 280.21s 
-- epoch 32 
-- finished epoch in 659.50s 
-- train score: 0.76342460881935, cost 544.14s 
-- test score: 0.51055253158057, cost 268.83s 
-- epoch 33 
-- finished epoch in 650.49s 
-- train score: 0.76733641536273, cost 544.45s 
-- test score: 0.50860121187224, cost 269.00s 
-- epoch 34 
-- finished epoch in 654.20s 
-- train score: 0.76083367201788, cost 544.57s 
-- test score: 0.51147684091609, cost 268.93s 
-- epoch 35 
-- finished epoch in 651.89s 
-- train score: 0.77386456004877, cost 544.53s 
-- test score: 0.51245250077026, cost 269.04s 
-- epoch 36 
-- finished epoch in 650.37s 
-- train score: 0.77629038813249, cost 544.18s 
-- test score: 0.51363356269898, cost 270.25s 
-- epoch 37 
-- finished epoch in 649.23s 
-- train score: 0.78316145092461, cost 544.05s 
-- test score: 0.50798500564856, cost 268.75s 
-- epoch 38 
-- finished epoch in 650.51s 
-- train score: 0.78370757976021, cost 545.24s 
-- test score: 0.50937146965184, cost 268.62s 
-- epoch 39 
-- finished epoch in 654.89s 
-- train score: 0.79056594188173, cost 544.69s 
-- test score: 0.51150251617541, cost 268.89s 
-- epoch 40 
-- finished epoch in 650.93s 
-- train score: 0.79245834180045, cost 544.18s 
-- test score: 0.51088630995173, cost 268.80s 
-- epoch 41 
-- finished epoch in 649.37s 
-- train score: 0.79506197927251, cost 544.50s 
-- test score: 0.50914039231796, cost 269.93s 
-- epoch 42 
-- finished epoch in 650.32s 
-- train score: 0.79386811623654, cost 543.81s 
-- test score: 0.51157954195337, cost 268.70s 
-- epoch 43 
-- finished epoch in 649.40s 
-- train score: 0.79366490550701, cost 544.32s 
-- test score: 0.51037280476533, cost 268.66s 
-- epoch 44 
-- finished epoch in 654.13s 
-- train score: 0.79996443812233, cost 544.26s 
-- test score: 0.50921741809592, cost 268.81s 
-- epoch 45 
-- finished epoch in 654.39s 
-- train score: 0.80151391993497, cost 544.21s 
-- test score: 0.50819040772312, cost 268.80s 
-- epoch 46 
-- finished epoch in 650.47s 
-- train score: 0.79842765698029, cost 543.92s 
-- test score: 0.50310670637773, cost 268.70s 
-- epoch 47 
-- finished epoch in 649.44s 
-- train score: 0.80527331843121, cost 544.14s 
-- test score: 0.50703502105371, cost 268.65s 
-- epoch 48 
-- finished epoch in 653.79s 
-- train score: 0.80714031700874, cost 544.01s 
-- test score: 0.50896066550272, cost 268.86s 
-- epoch 49 
-- finished epoch in 650.26s 
-- train score: 0.80773724852672, cost 544.03s 
-- test score: 0.50505802608606, cost 268.69s 
-- epoch 50 
-- finished epoch in 651.01s 
-- train score: 0.81631020117862, cost 544.90s 
-- test score: 0.50598233542159, cost 268.60s 
finished training in 73516.16s 
best dev score is: 0.51745917633768 
writing model to ./done/vqalstm-COCOQA-lstm.l1.d150.e17.c1-2015-12-18T140051.t7 
